{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":94327,"sourceType":"datasetVersion","datasetId":50445}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport math\nimport random\nfrom collections import defaultdict\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nfrom torch.optim import Adam\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom sklearn.metrics import classification_report, f1_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:15:33.610200Z","iopub.execute_input":"2024-12-08T09:15:33.610613Z","iopub.status.idle":"2024-12-08T09:15:33.617298Z","shell.execute_reply.started":"2024-12-08T09:15:33.610579Z","shell.execute_reply":"2024-12-08T09:15:33.616103Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"data_path = '/kaggle/input/conll003-englishversion/'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:15:33.620136Z","iopub.execute_input":"2024-12-08T09:15:33.620605Z","iopub.status.idle":"2024-12-08T09:15:33.634543Z","shell.execute_reply.started":"2024-12-08T09:15:33.620558Z","shell.execute_reply":"2024-12-08T09:15:33.633184Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"def load_sentences(filepath):\n    final = []\n    sentences = []\n    with open(filepath, 'r') as f:\n        for line in f.readlines():\n            if line == '-DOCSTART- -X- -X- O\\n' or line == '\\n':\n                if len(sentences) > 0:\n                    final.append(sentences)\n                    sentences = []\n            else:\n                l = line.split(' ')\n                sentences.append((l[0], l[3].strip('\\n')))\n    return final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:15:33.635924Z","iopub.execute_input":"2024-12-08T09:15:33.636251Z","iopub.status.idle":"2024-12-08T09:15:33.648815Z","shell.execute_reply.started":"2024-12-08T09:15:33.636219Z","shell.execute_reply":"2024-12-08T09:15:33.647590Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"train_sents = load_sentences(data_path + 'train.txt')\ntest_sents = load_sentences(data_path + 'test.txt')\nval_sents = load_sentences(data_path + 'valid.txt')\n\ntrain_sents[:3]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:15:33.649915Z","iopub.execute_input":"2024-12-08T09:15:33.650219Z","iopub.status.idle":"2024-12-08T09:15:34.137724Z","shell.execute_reply.started":"2024-12-08T09:15:33.650180Z","shell.execute_reply":"2024-12-08T09:15:34.136305Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"[[('EU', 'B-ORG'),\n  ('rejects', 'O'),\n  ('German', 'B-MISC'),\n  ('call', 'O'),\n  ('to', 'O'),\n  ('boycott', 'O'),\n  ('British', 'B-MISC'),\n  ('lamb', 'O'),\n  ('.', 'O')],\n [('Peter', 'B-PER'), ('Blackburn', 'I-PER')],\n [('BRUSSELS', 'B-LOC'), ('1996-08-22', 'O')]]"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"ner_labels = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\nid2label = {str(i): label for i, label in enumerate(ner_labels)}\nlabel2id = {value: int(key) for key, value in id2label.items()}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:15:34.141044Z","iopub.execute_input":"2024-12-08T09:15:34.141591Z","iopub.status.idle":"2024-12-08T09:15:34.149730Z","shell.execute_reply.started":"2024-12-08T09:15:34.141532Z","shell.execute_reply":"2024-12-08T09:15:34.148119Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"def get_df(samples):\n    df,label = [], []\n    for lines in samples:\n        cur_line, cur_label = list(zip(*lines))\n        df.append(list(cur_line))\n        label.append([label2id[i] for i in cur_label])\n    return {'text':df, 'label':label}\n    \n    \ntrain_df = get_df(train_sents)\ntest_df = get_df(test_sents)\nval_df = get_df(val_sents)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:15:34.151142Z","iopub.execute_input":"2024-12-08T09:15:34.151664Z","iopub.status.idle":"2024-12-08T09:15:34.278120Z","shell.execute_reply.started":"2024-12-08T09:15:34.151604Z","shell.execute_reply":"2024-12-08T09:15:34.277099Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"word_dict = defaultdict(int)\n\nfor line in train_df['text']:\n    for word in line:\n        word_dict[word] += 1\n\nlower_freq_word = []\nfor k,v in word_dict.items():\n    if v < 2:\n        lower_freq_word.append(k)\n\nfor word in lower_freq_word:\n    del word_dict[word]\n    \nword_dict['<UNK>'] = -1\nword_dict['<PAD>'] = -2\n\nword2id = {}\n\nfor idx, word in enumerate(word_dict.keys()):\n    word2id[word] = idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:15:34.279446Z","iopub.execute_input":"2024-12-08T09:15:34.279784Z","iopub.status.idle":"2024-12-08T09:15:34.375045Z","shell.execute_reply.started":"2024-12-08T09:15:34.279753Z","shell.execute_reply":"2024-12-08T09:15:34.373937Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"def prepare_sequence(seq, to_ix):\n    idxs = []\n    for w in seq:\n        if w in to_ix.keys():\n            idxs.append(to_ix[w])\n        else:\n            idxs.append(to_ix['<UNK>'])\n    return idxs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:16:14.890023Z","iopub.execute_input":"2024-12-08T09:16:14.890823Z","iopub.status.idle":"2024-12-08T09:16:14.897022Z","shell.execute_reply.started":"2024-12-08T09:16:14.890780Z","shell.execute_reply":"2024-12-08T09:16:14.895984Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"class CoNLLDataset(Dataset):\n    def __init__(self, df):\n        self.texts = df['text']\n        self.labels = df['label']\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_sequence(self.texts[item], word2id)\n        label = self.labels[item]\n        return {\n            'input_ids': inputs,\n            'labels': label\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:16:35.672485Z","iopub.execute_input":"2024-12-08T09:16:35.672859Z","iopub.status.idle":"2024-12-08T09:16:35.679113Z","shell.execute_reply.started":"2024-12-08T09:16:35.672829Z","shell.execute_reply":"2024-12-08T09:16:35.677924Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"class Collate:\n    def __init__(self, train):\n        self.train = train\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n        if self.train:\n            output[\"labels\"] = [sample[\"labels\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n\n        # add padding\n\n        output[\"input_ids\"] = [s + (batch_max - len(s)) * [word2id['<PAD>']] for s in output[\"input_ids\"]]\n        if self.train:\n            output['labels'] = [s + (batch_max - len(s)) * [-100] for s in output[\"labels\"]]\n\n        # convert to tensors\n        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n        if self.train:\n            output[\"labels\"] = torch.tensor(output[\"labels\"], dtype=torch.long)\n\n        return output\n    \ncollate_fn = Collate(True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:17:21.452160Z","iopub.execute_input":"2024-12-08T09:17:21.452650Z","iopub.status.idle":"2024-12-08T09:17:21.461589Z","shell.execute_reply.started":"2024-12-08T09:17:21.452579Z","shell.execute_reply":"2024-12-08T09:17:21.460267Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"class BiLSTMTagger(nn.Module):\n\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, output_size, embeddings=None):\n        super(BiLSTMTagger, self).__init__()\n        \n        # 1. Embedding Layer\n        if embeddings is None:\n            self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        else:\n            self.embeddings = nn.Embedding.from_pretrained(embeddings)\n        \n        # 2. LSTM Layer\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, num_layers=3, batch_first=True)\n\n        # 3. Dense Layer\n        self.fc = nn.Linear(2*hidden_dim, output_size)\n        \n    def forward(self, batch_text):\n\n        embeddings = self.embeddings(batch_text)\n        \n        lstm_output, _ = self.lstm(embeddings) \n\n        logits = self.fc(lstm_output)\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:18:33.369579Z","iopub.execute_input":"2024-12-08T09:18:33.369973Z","iopub.status.idle":"2024-12-08T09:18:33.378094Z","shell.execute_reply.started":"2024-12-08T09:18:33.369941Z","shell.execute_reply":"2024-12-08T09:18:33.376447Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"def remove_predictions_for_masked_items(predicted_labels, correct_labels): \n\n    predicted_labels_without_mask = []\n    correct_labels_without_mask = []\n        \n    for p, c in zip(predicted_labels, correct_labels):\n        if c > 0:\n            predicted_labels_without_mask.append(p)\n            correct_labels_without_mask.append(c)\n            \n    return predicted_labels_without_mask, correct_labels_without_mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:19:56.069363Z","iopub.execute_input":"2024-12-08T09:19:56.069757Z","iopub.status.idle":"2024-12-08T09:19:56.075576Z","shell.execute_reply.started":"2024-12-08T09:19:56.069724Z","shell.execute_reply":"2024-12-08T09:19:56.074435Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"def train(model, train_loader, val_loader, batch_size, max_epochs, num_batches, patience, output_path):\n    criterion = nn.CrossEntropyLoss(ignore_index=-100)  # we mask the <pad> labels\n    optimizer = Adam(model.parameters())\n\n    train_f_score_history = []\n    dev_f_score_history = []\n    no_improvement = 0\n    for epoch in range(max_epochs):\n\n        total_loss = 0\n        predictions, correct = [], []\n        model.train()\n        for batch in tqdm(train_loader, total=num_batches, desc=f\"Epoch {epoch}\"):\n            \n            cur_batch_size, text_length = batch['input_ids'].shape\n            \n            pred = model(batch['input_ids'].to(device)).view(cur_batch_size*text_length, NUM_CLASSES)\n            gold = batch['labels'].to(device).view(cur_batch_size*text_length)\n            \n            loss = criterion(pred, gold)\n            \n            total_loss += loss.item()\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            _, pred_indices = torch.max(pred, 1)\n            \n            predicted_labels = list(pred_indices.cpu().numpy())\n            correct_labels = list(batch['labels'].view(cur_batch_size*text_length).numpy())\n            \n            predicted_labels, correct_labels = remove_predictions_for_masked_items(predicted_labels, \n                                                                                   correct_labels)\n            \n            predictions += predicted_labels\n            correct += correct_labels\n\n        train_score = f1_score(correct, predictions, average=\"macro\")\n        train_f_score_history.append(train_score)\n            \n        print(\"Total training loss:\", total_loss)\n        print(\"Training Macro F1:\", train_score)\n        \n        total_loss = 0\n        predictions, correct = [], []\n        \n        model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n\n                cur_batch_size, text_length = batch['input_ids'].shape\n                \n                pred = model(batch['input_ids'].to(device)).view(cur_batch_size*text_length, NUM_CLASSES)\n                gold = batch['labels'].to(device).view(cur_batch_size*text_length)\n                \n                loss = criterion(pred, gold)\n                total_loss += loss.item()\n\n                _, pred_indices = torch.max(pred, 1)\n                predicted_labels = list(pred_indices.cpu().numpy())\n                correct_labels = list(batch['labels'].view(cur_batch_size*text_length).numpy())\n\n                predicted_labels, correct_labels = remove_predictions_for_masked_items(predicted_labels, \n                                                                                       correct_labels)\n\n                predictions += predicted_labels\n                correct += correct_labels\n\n        dev_score = f1_score(correct, predictions, average=\"macro\")\n            \n        print(\"Total validation loss:\", total_loss)\n        print(\"Validation Macro F1:\", dev_score)\n        \n        dev_f = dev_score\n        if len(dev_f_score_history) > patience and dev_f < max(dev_f_score_history):\n            no_improvement += 1\n\n        elif len(dev_f_score_history) == 0 or dev_f > max(dev_f_score_history):\n            print(\"Saving model.\")\n            torch.save(model, output_path)\n            no_improvement = 0\n            \n        if no_improvement > patience:\n            print(\"Validation F-score does not improve anymore. Stop training.\")\n            dev_f_score_history.append(dev_f)\n            break\n            \n        dev_f_score_history.append(dev_f)\n        \n    return train_f_score_history, dev_f_score_history\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:20:38.681649Z","iopub.execute_input":"2024-12-08T09:20:38.682135Z","iopub.status.idle":"2024-12-08T09:20:38.706434Z","shell.execute_reply.started":"2024-12-08T09:20:38.682089Z","shell.execute_reply":"2024-12-08T09:20:38.705173Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"def test(model, test_iter, batch_size, labels, target_names): \n    total_loss = 0\n    predictions, correct = [], []\n    \n    model.eval()\n    with torch.no_grad():    \n    \n        for batch in test_iter:\n\n            cur_batch_size, text_length = batch['input_ids'].shape\n\n            pred = model(batch['input_ids'].to(device)).view(cur_batch_size*text_length, NUM_CLASSES)\n            gold = batch['labels'].to(device).view(cur_batch_size*text_length)\n\n            _, pred_indices = torch.max(pred, 1)\n            predicted_labels = list(pred_indices.cpu().numpy())\n            correct_labels = list(batch['labels'].view(cur_batch_size*text_length).numpy())\n\n            predicted_labels, correct_labels = remove_predictions_for_masked_items(predicted_labels, \n                                                                                   correct_labels)\n\n            predictions += predicted_labels\n            correct += correct_labels\n    \n    print(classification_report(correct, predictions, labels=labels, target_names=target_names))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:21:20.487485Z","iopub.execute_input":"2024-12-08T09:21:20.488261Z","iopub.status.idle":"2024-12-08T09:21:20.495797Z","shell.execute_reply.started":"2024-12-08T09:21:20.488220Z","shell.execute_reply":"2024-12-08T09:21:20.494568Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"EMBEDDING_DIM = 100\nHIDDEN_DIM = 64\nNUM_CLASSES = len(id2label)\nMAX_EPOCHS = 50\nPATIENCE = 3\nBATCH_SIZE = 32\nVOCAB_SIZE = len(word2id)\nOUTPUT_PATH = \"/tmp/bilstmtagger\"\nnum_batches = math.ceil(len(train_df) / BATCH_SIZE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:21:43.753974Z","iopub.execute_input":"2024-12-08T09:21:43.754725Z","iopub.status.idle":"2024-12-08T09:21:43.759719Z","shell.execute_reply.started":"2024-12-08T09:21:43.754680Z","shell.execute_reply":"2024-12-08T09:21:43.758656Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"train_dataset = CoNLLDataset(train_df)\nval_dataset = CoNLLDataset(val_df)\ntest_dataset = CoNLLDataset(test_df)\n\ntrain_loader = DataLoader(train_dataset,\n                              batch_size=BATCH_SIZE,\n                              shuffle=False,\n                              collate_fn=collate_fn,\n                              num_workers=4,\n                              pin_memory=True,\n                              drop_last=False)\n\nval_loader = DataLoader(val_dataset,\n                              batch_size=BATCH_SIZE,\n                              shuffle=False,\n                              collate_fn=collate_fn,\n                              num_workers=4,\n                              pin_memory=True,\n                              drop_last=False)\n\ntest_loader = DataLoader(test_dataset,\n                              batch_size=BATCH_SIZE,\n                              shuffle=False,\n                              collate_fn=collate_fn,\n                              num_workers=4,\n                              pin_memory=True,\n                              drop_last=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:22:03.113207Z","iopub.execute_input":"2024-12-08T09:22:03.113632Z","iopub.status.idle":"2024-12-08T09:22:03.121418Z","shell.execute_reply.started":"2024-12-08T09:22:03.113586Z","shell.execute_reply":"2024-12-08T09:22:03.119767Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"tagger = BiLSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE+2, NUM_CLASSES) \ntagger\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:22:11.668214Z","iopub.execute_input":"2024-12-08T09:22:11.668615Z","iopub.status.idle":"2024-12-08T09:22:11.728102Z","shell.execute_reply.started":"2024-12-08T09:22:11.668582Z","shell.execute_reply":"2024-12-08T09:22:11.726923Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"BiLSTMTagger(\n  (embeddings): Embedding(11986, 100)\n  (lstm): LSTM(100, 64, num_layers=3, batch_first=True, bidirectional=True)\n  (fc): Linear(in_features=128, out_features=9, bias=True)\n)"},"metadata":{}}],"execution_count":62},{"cell_type":"code","source":"train_f, dev_f = train(tagger.to(device), train_loader, val_loader, BATCH_SIZE, MAX_EPOCHS, \n                       num_batches, PATIENCE, OUTPUT_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:22:25.386072Z","iopub.execute_input":"2024-12-08T09:22:25.386632Z","iopub.status.idle":"2024-12-08T09:31:19.263743Z","shell.execute_reply.started":"2024-12-08T09:22:25.386592Z","shell.execute_reply":"2024-12-08T09:31:19.262338Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5a0ff98bda24f95a81098311bd67490"}},"metadata":{}},{"name":"stdout","text":"Total training loss: 344.91125263273716\nTraining Macro F1: 0.015759030414735937\nTotal validation loss: 56.12493021786213\nValidation Macro F1: 0.12278908939995542\nSaving model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c196e995ab34cbda42b92fe665b6a0e"}},"metadata":{}},{"name":"stdout","text":"Total training loss: 169.40841363370419\nTraining Macro F1: 0.34511809616674805\nTotal validation loss: 32.19121523946524\nValidation Macro F1: 0.3909965080443668\nSaving model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a8334a7aadb408a9b5cd634a5d87267"}},"metadata":{}},{"name":"stdout","text":"Total training loss: 94.49006951414049\nTraining Macro F1: 0.5633081707952685\nTotal validation loss: 24.53801471926272\nValidation Macro F1: 0.5754467136828897\nSaving model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa04204eda624fc2aadb780efb190306"}},"metadata":{}},{"name":"stdout","text":"Total training loss: 57.38383488636464\nTraining Macro F1: 0.6982772593818211\nTotal validation loss: 19.97235928615555\nValidation Macro F1: 0.6620841531433297\nSaving model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7544f5807f64d22986afc2b9797b30a"}},"metadata":{}},{"name":"stdout","text":"Total training loss: 36.80419745715335\nTraining Macro F1: 0.7702704142394489\nTotal validation loss: 18.357784854946658\nValidation Macro F1: 0.6912155592611796\nSaving model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"531ce676bcde454dadc5be1d665f7310"}},"metadata":{}},{"name":"stdout","text":"Total training loss: 24.71564652340021\nTraining Macro F1: 0.8143480469290479\nTotal validation loss: 18.344960731221363\nValidation Macro F1: 0.7007456356658951\nSaving model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"389e9bbeaf864a9ca7e9f61dabbfe6b9"}},"metadata":{}},{"name":"stdout","text":"Total training loss: 18.02895562676713\nTraining Macro F1: 0.8380445039382818\nTotal validation loss: 18.702265097934287\nValidation Macro F1: 0.7053333844190042\nSaving model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3be75b3988d545af88602c6dcc7834a2"}},"metadata":{}},{"name":"stdout","text":"Total training loss: 12.79710330144735\nTraining Macro F1: 0.8544786054049311\nTotal validation loss: 19.24992256623227\nValidation Macro F1: 0.7079567246533347\nSaving model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c41a19125124a50b0db89f3b4fe29a6"}},"metadata":{}},{"name":"stdout","text":"Total training loss: 9.996878715639468\nTraining Macro F1: 0.8638350364609675\nTotal validation loss: 20.82966603297973\nValidation Macro F1: 0.7128380770052578\nSaving model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dd7d372b60b4077925d584394596be1"}},"metadata":{}},{"name":"stdout","text":"Total training loss: 8.351324024581118\nTraining Macro F1: 0.867895969663346\nTotal validation loss: 20.563435456497245\nValidation Macro F1: 0.7179940495111077\nSaving model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c0e32ee56eb482c910c1e3a6f13c35b"}},"metadata":{}},{"name":"stdout","text":"Total training loss: 7.492537721525878\nTraining Macro F1: 0.8697717958699429\nTotal validation loss: 21.29669574102445\nValidation Macro F1: 0.7193381615401253\nSaving model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac2ffee07a1f40a89af6494f43332c77"}},"metadata":{}},{"name":"stdout","text":"Total training loss: 6.228401094733272\nTraining Macro F1: 0.8728223420514779\nTotal validation loss: 21.214801813548547\nValidation Macro F1: 0.7293125129767131\nSaving model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"678118877c984f46905a7bfedfd01962"}},"metadata":{}},{"name":"stdout","text":"Total training loss: 4.595958497564425\nTraining Macro F1: 0.8775759580764044\nTotal validation loss: 21.35615302968654\nValidation Macro F1: 0.7284069577117012\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a58c9ec454b4556a534c373362388c2"}},"metadata":{}},{"name":"stdout","text":"Total training loss: 3.822086682092049\nTraining Macro F1: 0.8797246720832006\nTotal validation loss: 22.77894842135356\nValidation Macro F1: 0.7229216199709314\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0be12faf5a64b34824b530eddfa2e54"}},"metadata":{}},{"name":"stdout","text":"Total training loss: 3.4123341184549645\nTraining Macro F1: 0.8814040693502462\nTotal validation loss: 23.280070661312493\nValidation Macro F1: 0.7312843020522896\nSaving model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a06979e5b5f4a9f9f0432673a2f1b3e"}},"metadata":{}},{"name":"stdout","text":"Total training loss: 3.04851106451315\nTraining Macro F1: 0.8820989537208375\nTotal validation loss: 23.38572436825416\nValidation Macro F1: 0.7312267384319441\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9e36187cb2c4145abe1f482f95f5416"}},"metadata":{}},{"name":"stdout","text":"Total training loss: 2.4467578478415817\nTraining Macro F1: 0.8831553081997341\nTotal validation loss: 25.135275926100803\nValidation Macro F1: 0.7289843343072056\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"179ef03e9d194490a8553a0cab683546"}},"metadata":{}},{"name":"stdout","text":"Total training loss: 1.93056082939529\nTraining Macro F1: 0.8849791436329109\nTotal validation loss: 27.75272669313017\nValidation Macro F1: 0.7249804365335615\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a16d8901e3a94092b0533a3cd4e1f805"}},"metadata":{}},{"name":"stdout","text":"Total training loss: 2.9225483823811373\nTraining Macro F1: 0.8814133408316237\nTotal validation loss: 26.422789808002562\nValidation Macro F1: 0.7277859207371847\nValidation F-score does not improve anymore. Stop training.\n","output_type":"stream"}],"execution_count":63}]}