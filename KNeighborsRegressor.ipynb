{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework_8 - KNeighborsRegressor for Salary Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on Wed Oct  2 11:16:45 2024\n",
    "@author: hanna.dunska\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Setting up the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 249 entries, 0 to 248\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Name           249 non-null    object \n",
      " 1   Phone_Number   249 non-null    object \n",
      " 2   Experience     247 non-null    float64\n",
      " 3   Qualification  248 non-null    object \n",
      " 4   University     249 non-null    object \n",
      " 5   Role           246 non-null    object \n",
      " 6   Cert           247 non-null    object \n",
      " 7   Date_Of_Birth  249 non-null    object \n",
      " 8   Salary         249 non-null    int64  \n",
      "dtypes: float64(1), int64(1), object(7)\n",
      "memory usage: 17.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Name           7 non-null      object\n",
      " 1   Phone_Number   7 non-null      object\n",
      " 2   Experience     7 non-null      int64 \n",
      " 3   Qualification  7 non-null      object\n",
      " 4   University     7 non-null      object\n",
      " 5   Role           7 non-null      object\n",
      " 6   Cert           7 non-null      object\n",
      " 7   Date_Of_Birth  7 non-null      object\n",
      " 8   Salary         7 non-null      int64 \n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 636.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv('/Users/hanna.dunska/Desktop/Machine Learning/MACHINE-LEARNING-NEO/datasets/mod_04_hw_train_data.csv')\n",
    "X_test = pd.read_csv('/Users/hanna.dunska/Desktop/Machine Learning/MACHINE-LEARNING-NEO/datasets/mod_04_hw_valid_data.csv')\n",
    "\n",
    "X_train.info()\n",
    "X_test.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Defining the Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    109300\n",
       "1     84800\n",
       "2     98900\n",
       "3    116500\n",
       "4     75800\n",
       "Name: Salary, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = X_train.pop('Salary')\n",
    "y_train.head()\n",
    "\n",
    "y_test = X_test.pop('Salary')\n",
    "y_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age           Age_Group\n",
      "0   49     45-54 years old\n",
      "1    8  Under 18 years old\n",
      "2   19     18-24 years old\n",
      "3   51     45-54 years old\n",
      "4   51     45-54 years old\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 220 entries, 0 to 248\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Experience     220 non-null    float64\n",
      " 1   Qualification  220 non-null    object \n",
      " 2   University     220 non-null    object \n",
      " 3   Role           220 non-null    object \n",
      " 4   Cert           220 non-null    object \n",
      " 5   Age_Group      220 non-null    object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 12.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Experience     7 non-null      int64 \n",
      " 1   Qualification  7 non-null      object\n",
      " 2   University     7 non-null      object\n",
      " 3   Role           7 non-null      object\n",
      " 4   Cert           7 non-null      object\n",
      " 5   Age_Group      7 non-null      object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 468.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Outliers\n",
    "X_train['Experience'].describe()\n",
    "# Decision_1: I did not perform outlier cleaning due to the absence of outliers\n",
    "\n",
    "# 4.2 Feature Engineering in X_train\n",
    "X_train['Date_Of_Birth'].head()\n",
    "\n",
    "X_train['Date_Of_Birth'] = pd.to_datetime(\n",
    "    X_train['Date_Of_Birth'], \n",
    "    format='%d/%m/%Y', \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "if X_train['Date_Of_Birth'].isna().sum() > 0:\n",
    "    print(\"There are NaT values in 'Date_Of_Birth'.\")    \n",
    "    \n",
    "X_train['Date_Of_Birth'].head()\n",
    "\n",
    "max_year = X_train['Date_Of_Birth'].dt.year.max()\n",
    "max_year\n",
    "\n",
    "X_train['Age'] = max_year - X_train['Date_Of_Birth'].dt.year\n",
    "X_train['Age'].describe()\n",
    "\n",
    "bins = [-1, 17, 24, 34, 44, 54, float('inf')]\n",
    "labels = ['Under 18 years old', '18-24 years old', '25-34 years old', '35-44 years old', '45-54 years old', '55 and older']\n",
    "\n",
    "X_train['Age_Group'] = pd.cut(X_train['Age'], bins=bins, labels=labels).astype('object')\n",
    "print(X_train[['Age', 'Age_Group']].head())\n",
    "\n",
    "\n",
    "X_train.drop(['Phone_Number', 'Name', 'Age', 'Date_Of_Birth'], axis=1, inplace=True)\n",
    "X_train = X_train[X_train['Age_Group'] != 'Under 18 years old']\n",
    "y_train = y_train[X_train.index] \n",
    "# Decision_2: I deleted the values of the 'Under 18 years old' group due to legal concerns.\n",
    "# Decision_3: I have made a decision to delete the 'Phone_Number', 'Name', 'Age', 'Date_Of_Birth' features as unnecessary ones\n",
    "\n",
    "# 4.3 Handling NAs in X_train\n",
    "X_train.isna().mean().sort_values(ascending=False)\n",
    "X_train.dropna(how='any', inplace=True)\n",
    "y_train = y_train[X_train.index] \n",
    "# Decision_4: I have decided to remove NA values from the dataset and not to impute any data (such as average values or interpolated values)\n",
    "\n",
    "# 4.4 Feature Engineering in X_test\n",
    "X_test['Date_Of_Birth'] = pd.to_datetime(\n",
    "    X_test['Date_Of_Birth'], \n",
    "    format='%d/%m/%Y', \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "X_test['Age'] = max_year - X_test['Date_Of_Birth'].dt.year\n",
    "X_test['Age_Group'] = pd.cut(X_test['Age'], bins=bins, labels=labels).astype('object')\n",
    "\n",
    "\n",
    "X_test.drop(['Phone_Number', 'Name', 'Age', 'Date_Of_Birth'], axis=1, inplace=True)\n",
    "X_test = X_test[X_test['Age_Group'] != 'Under 18 years old']\n",
    "y_train = y_train[X_train.index]\n",
    "\n",
    "\n",
    "# 4.5 Handling NAs in X_test\n",
    "X_test.isna().mean().sort_values(ascending=False)\n",
    "X_test.dropna(how='any', inplace=True)\n",
    "\n",
    "X_train.info()\n",
    "X_test.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Discretizing Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.        , 2.33333333, 3.66666667, 5.        ])]\n"
     ]
    }
   ],
   "source": [
    "num_cols = X_train.select_dtypes(include='float64').columns\n",
    "kbins = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform').fit(X_train[num_cols])\n",
    "\n",
    "X_train[num_cols] = kbins.transform(X_train[num_cols]).astype(int)\n",
    "X_test[num_cols] = kbins.transform(X_test[num_cols]).astype(int)\n",
    "\n",
    "print(kbins.bin_edges_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Categorical DataProcessing and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', category=FutureWarning)\n",
    "\n",
    "    ordinal_cols = ['Experience', 'Qualification'] \n",
    "    onehot_cols = ['University', 'Role', 'Cert', 'Age_Group']  \n",
    "    \n",
    "    ordinal_encoder = ce.OrdinalEncoder(cols=ordinal_cols)\n",
    "    X_train = ordinal_encoder.fit_transform(X_train)\n",
    "    X_test = ordinal_encoder.transform(X_test)\n",
    "    \n",
    "    onehot_encoder = ce.OneHotEncoder(cols=onehot_cols, use_cat_names=True)\n",
    "    X_train = onehot_encoder.fit_transform(X_train, y_train)\n",
    "    X_test = onehot_encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7. Assimetry Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_transform = PowerTransformer().set_output(transform='pandas')\n",
    "\n",
    "X_train = power_transform.fit_transform(X_train)\n",
    "X_test = power_transform.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8. Constructing the KNeighbors Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAPE: 4.28%\n"
     ]
    }
   ],
   "source": [
    "knn_reg_mod = KNeighborsRegressor(n_neighbors=10, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "knn_reg_preds = knn_reg_mod.predict(X_test)\n",
    "\n",
    "mape = mean_absolute_percentage_error(y_test, knn_reg_preds)\n",
    "print(f'Validation MAPE: {mape:.2%}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the process of EDA, I made the following decisions:\n",
    "\n",
    " - Decision 1: I did not perform outlier cleaning for the float data, particularly the \"Experience\" feature, due to the absence of outliers.\n",
    " - Decision 2: I decided to delete the 'Phone_Number', 'Name', 'Age', and 'Date_Of_Birth' features as unnecessary.\n",
    " - Decision 3: I deleted the values of the 'Under 18 years old' group due to legal concerns - minors cannot work, and thus there can be no talk of predicting salary.\n",
    " - Decision 4: I decided to remove NA values from the dataset and not to impute any data (such as average values or interpolated values) in order not to introduce noise into the data.\n",
    "\n",
    "I carried out these steps for both the training and test datasets.\n",
    "\n",
    "Next, I used KBinsDiscretizer(n_bins=3, strategy='uniform'). This discretization helped me generalize the experience variable. My experience was distributed from 1 to 5, so I logically divided it into the following bins:\n",
    "\n",
    " - 1 to 2.33\n",
    " - 2.33 to 3.67\n",
    " - 3.67 to 5\n",
    "\n",
    "Then, I used different category encoders:\n",
    "\n",
    " - Ordinal columns: ['Experience', 'Qualification']\n",
    " - One-hot columns: ['University', 'Role', 'Cert', 'Age_Group']\n",
    "\n",
    "For ordinal categorical variables, I used OrdinalEncoder, while for categorical variables without an explicit order, I used OneHotEncoder.\n",
    "\n",
    "By using PowerTransformer, I reduced the skewness of numerical variables since all variables became numerical after encoding.\n",
    "\n",
    "Then, I found that in my case, the optimal number of neighbors is 10 for the KNeighborsRegressor.\n",
    "\n",
    "The obtained result corresponds to the task conditions, as I expect to achieve a MAPE error on the validation set in the range of 3-5%."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
